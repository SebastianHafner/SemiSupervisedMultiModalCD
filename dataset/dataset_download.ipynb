{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to create the SEN12 Multi-Temporal Urban Mapping Dataset\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all packages\n",
    "import rasterio\n",
    "import json\n",
    "from pathlib import Path\n",
    "import math\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ee\n",
    "\n",
    "# initialize Google Earth Engine Python API (requires an authorized GEE account)\n",
    "ee.Initialize()\n",
    "\n",
    "# path to the SpaceNet 7 dataset: https://spacenet.ai/sn7-challenge/\n",
    "sn7_path = Path('C:/Users/shafner/datasets/spacenet7')\n",
    "# this folder should contain the metadata files for the train (metadata_train.json) and test (metadata_test.json) files\n",
    "# and the Sentinel-1 SAR orbits file (s1_orbit_numbers.json).\n",
    "# these files can be downloaded from the dataset folder in this git repository:\n",
    "# https://github.com/SebastianHafner/SemiSupervisedMultiModalCD.git\n",
    "\n",
    "# path to the SEN12 Multi-Temporal Urban Mapping Dataset\n",
    "new_dataset_folder =  Path('C:/Users/shafner/datasets/multimodal_cd_dataset')\n",
    "\n",
    "# define coordinate reference system\n",
    "crs = 'EPSG:3857'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and write functions for GeoTIFFs and JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in geotiff file as numpy array\n",
    "def read_tif(file: Path):\n",
    "    if not file.exists():\n",
    "        raise FileNotFoundError(f'File {file} not found')\n",
    "\n",
    "    with rasterio.open(file) as dataset:\n",
    "        arr = dataset.read()  # (bands X height X width)\n",
    "        transform = dataset.transform\n",
    "        crs = dataset.crs\n",
    "\n",
    "    return arr.transpose((1, 2, 0)), transform, crs\n",
    "\n",
    "def load_json(file: Path):\n",
    "    with open(str(file)) as f:\n",
    "        d = json.load(f)\n",
    "    return d\n",
    "\n",
    "def write_json(file: Path, data):\n",
    "    with open(str(file), 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get extent of the SpaceNet 7 study sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentinel_extent(dataset: str, aoi_id: str) -> ee.Feature:\n",
    "    metadata_file = sn7_path / f'metadata_{dataset}.json'\n",
    "    metadata = load_json(metadata_file)\n",
    "    timeseries = metadata[aoi_id]\n",
    "    year, month = timeseries[0]['year'], timeseries[0]['month']\n",
    "\n",
    "    img_file = sn7_path / dataset / aoi_id / 'images_masked' / f'global_monthly_{year}_{month:02d}_mosaic_{aoi_id}.tif'\n",
    "    img, transform, crs = read_tif(img_file)\n",
    "\n",
    "    m, n, _ = img.shape\n",
    "    x_res, _, x_min, _, y_res, y_max, *_ = transform\n",
    "\n",
    "    x_max = x_min + n * x_res\n",
    "    y_min = y_max + m * y_res\n",
    "    x_min = int(math.ceil(x_min / 10) * 10)\n",
    "    y_min = int(math.ceil(y_min / 10) * 10)\n",
    "\n",
    "    n = int(x_max - x_min) // 10\n",
    "    m = int(y_max - y_min) // 10\n",
    "    x_max = x_min + n * 10\n",
    "    y_max = y_min + m * 10\n",
    "    \n",
    "    crs == str(crs)\n",
    "    assert(crs == 'EPSG:3857')\n",
    "    \n",
    "    bbox = ee.Geometry.Rectangle(\n",
    "      coords=[x_min, y_min, x_max, y_max],\n",
    "      proj=str(crs),\n",
    "      geodesic=False,\n",
    "    );\n",
    "    \n",
    "    return ee.Feature(bbox, {'aoi_id': aoi_id}).transform('EPSG:4326', 0.001)\n",
    "\n",
    "\n",
    "dataset = 'train'\n",
    "\n",
    "metadata_file = sn7_path / f'metadata_{dataset}.json'\n",
    "metadata = load_json(metadata_file)\n",
    "aoi_ids = metadata.keys()\n",
    "\n",
    "extents = []\n",
    "for i, aoi_id in enumerate(aoi_ids):\n",
    "    extent = get_sentinel_extent(dataset, aoi_id)\n",
    "    extents.append(extent)\n",
    "    if i == 0:\n",
    "        pass\n",
    "\n",
    "fc = ee.FeatureCollection(extents)\n",
    "dl_task = ee.batch.Export.table.toDrive(\n",
    "    collection=fc,\n",
    "    description='test',\n",
    "    folder='sn7_extent',\n",
    "    fileNamePrefix=f'sn7_sites_{dataset}',\n",
    "    fileFormat='GeoJSON'\n",
    ")\n",
    "# dl_task.start()\n",
    "\n",
    "\n",
    "fc = ee.FeatureCollection(extents)\n",
    "dl_task = ee.batch.Export.table.toAsset(\n",
    "    collection=fc,\n",
    "    description=dataset,\n",
    "    assetId=f'users/hafnersailing/sn7_sites_{dataset}'\n",
    ")\n",
    "# dl_task.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Sentinel-1 SAR and Sentinel-2 MSI data pre-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentinel-2 MSI\n",
    "def add_cloud_score(img: ee.Image) -> ee.Image:\n",
    "    patch = ee.Geometry(img.get('patch'))\n",
    "    cloud_probability = ee.Image(img.get('cloudProbability'))\n",
    "    stats = cloud_probability.select('probability').reduceRegion(reducer=ee.Reducer.sum(),\n",
    "                                                                 geometry=patch,\n",
    "                                                                 scale=10,\n",
    "                                                                 maxPixels=1e12)\n",
    "    cloud_score = stats.get('probability')\n",
    "    img = img.set('cloudScore', cloud_score)\n",
    "    return img\n",
    "\n",
    "\n",
    "def mostly_cloud_free_mosaic(patch: ee.Geometry, start_date: ee.Date, end_date: ee.Date) -> ee.Image:\n",
    "    s2 = ee.ImageCollection('COPERNICUS/S2') \\\n",
    "        .filterDate(start_date, end_date) \\\n",
    "        .filterBounds(patch) \\\n",
    "        .map(lambda img: img.set('patch', patch))\n",
    "    s2_clouds = ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY') \\\n",
    "        .filterDate(start_date, end_date) \\\n",
    "        .filterBounds(patch)\n",
    "    join_condition = ee.Filter.equals(leftField='system:index', rightField='system:index')\n",
    "    s2 = ee.Join.saveFirst('cloudProbability').apply(primary=s2, secondary=s2_clouds, condition=join_condition)\n",
    "    s2 = s2.map(add_cloud_score)\n",
    "    s2 = ee.ImageCollection(s2)\n",
    "    img = s2.sort('cloudScore', False).mosaic()\n",
    "    img = img.unitScale(0, 10_000).clamp(0, 1)\n",
    "\n",
    "    return img\n",
    "\n",
    "# Sentinel-1 SAR\n",
    "def single_orbit_mean(patch: ee.Geometry, start_date: ee.Date, end_date: ee.Date, orbit_number: int = None) -> ee.Image:\n",
    "\n",
    "    # sup-setting data\n",
    "    col = ee.ImageCollection('COPERNICUS/S1_GRD') \\\n",
    "        .filterBounds(patch) \\\n",
    "        .filterDate(start_date, end_date) \\\n",
    "        .filterMetadata('instrumentMode', 'equals', 'IW') \\\n",
    "        .filterMetadata('transmitterReceiverPolarisation', 'equals', ['VV', 'VH'])\n",
    "\n",
    "    # masking noise\n",
    "    col = col.map(lambda img: img.updateMask(img.gte(-25)))\n",
    "    # print(col.size().getInfo())\n",
    "\n",
    "    # using orbit with more scenes if no orbit number is passed\n",
    "    if orbit_number is None:\n",
    "        asc_col = col.filterMetadata('orbitProperties_pass', 'equals', 'ASCENDING')\n",
    "        # print('asc', asc_col.size().getInfo())\n",
    "        desc_col = col.filterMetadata('orbitProperties_pass', 'equals', 'DESCENDING')\n",
    "        # print('desc', desc_col.size().getInfo())\n",
    "        col = ee.Algorithms.If(ee.Number(asc_col.size()).gt(desc_col.size()), asc_col, desc_col)\n",
    "        col = ee.ImageCollection(col)\n",
    "        # print(col.size().getInfo())\n",
    "        if col.size().getInfo() == 0:\n",
    "            return None\n",
    "\n",
    "        # getting distinct orbit numbers\n",
    "        orbit_numbers = col \\\n",
    "            .toList(col.size()) \\\n",
    "            .map(lambda img: ee.Number(ee.Image(img).get('relativeOrbitNumber_start'))) \\\n",
    "            .distinct().getInfo()\n",
    "\n",
    "        # computing separate mean backscatter image for each orbit number\n",
    "        means = ee.ImageCollection([])\n",
    "        for number in orbit_numbers:\n",
    "            mean = col.filterMetadata('relativeOrbitNumber_start', 'equals', number).mean()\n",
    "            means = means.merge(ee.ImageCollection([mean]))\n",
    "\n",
    "        img = means.mosaic()\n",
    "    else:\n",
    "        col = col.filterMetadata('relativeOrbitNumber_start', 'equals', orbit_number)\n",
    "        if col.size().getInfo() == 0:\n",
    "            return None\n",
    "        img = col.mean()\n",
    "\n",
    "    img = img.unitScale(-25, 0).clamp(0, 1)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the Sentinel data to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = 'train'\n",
    "\n",
    "orbit_numbers_file = sn7_path / 's1_orbit_numbers.json'\n",
    "orbit_numbers = load_json(orbit_numbers_file)\n",
    "metadata_file = sn7_path / f'metadata_{dataset}.json'\n",
    "metadata = load_json(metadata_file)\n",
    "aoi_ids = metadata.keys()\n",
    "\n",
    "for i, aoi_id in enumerate(aoi_ids):\n",
    "    timeseries = metadata[aoi_id]\n",
    "    orbit_number = orbit_numbers[aoi_id] if aoi_id in orbit_numbers.keys() else None\n",
    "    for timestamp in timeseries:\n",
    "        year = timestamp['year']\n",
    "        month = timestamp['month']\n",
    "        start_date = ee.Date(f'{year}-{month:02d}-01')\n",
    "        end_date = start_date.advance(1, 'month')\n",
    "        extent = get_sentinel_extent(dataset, aoi_id)\n",
    "        \n",
    "        s2_img = mostly_cloud_free_mosaic(extent.geometry(), start_date, end_date)\n",
    "        s2_dl_task = ee.batch.Export.image.toDrive(\n",
    "            image=s2_img.select(['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12']).float(),\n",
    "            region=extent.geometry().getInfo()['coordinates'],\n",
    "            description=f'{aoi_id}_{year}_{month:02d}',\n",
    "            folder=f'multimodal_cd_dataset_{dataset}',\n",
    "            fileNamePrefix=f's2_{aoi_id}_{year}_{month:02d}',\n",
    "            scale=10,\n",
    "            crs=crs,\n",
    "            maxPixels=250_000,\n",
    "            fileFormat='GeoTIFF'\n",
    "        )\n",
    "        s2_dl_task.start()\n",
    "\n",
    "        s1_img = single_orbit_mean(extent.geometry(), start_date, end_date, orbit_number)\n",
    "        if s1_img is None:\n",
    "            continue\n",
    "        s1_dl_task = ee.batch.Export.image.toDrive(\n",
    "            image=s1_img.select(['VV', 'VH']).float(),\n",
    "            region=extent.geometry().getInfo()['coordinates'],\n",
    "            description=f'{aoi_id}_{year}_{month:02d}',\n",
    "            folder=f'multimodal_cd_dataset_{dataset}',\n",
    "            fileNamePrefix=f's1_{aoi_id}_{year}_{month:02d}',\n",
    "            scale=10,\n",
    "            crs=crs,\n",
    "            maxPixels=250_000,\n",
    "            fileFormat='GeoTIFF'\n",
    "        )\n",
    "        s1_dl_task.start()\n",
    "\n",
    "        if dataset == 'train':\n",
    "            building_footprints = ee.FeatureCollection(f'users/hafnersailing/spacenet7/buildings_{aoi_id}')\n",
    "            building_footprints = building_footprints \\\n",
    "                .filterMetadata('year', 'equals', year) \\\n",
    "                .filterMetadata('month', 'equals', month)\n",
    "            # print(f'n buildings: {building_footprints.size().getInfo()}')\n",
    "            building_footprints = building_footprints.map(lambda f: ee.Feature(f).set({'buildings': 1}))\n",
    "\n",
    "            buildings = building_footprints.reduceToImage(['buildings'], ee.Reducer.first()) \\\n",
    "                .unmask() \\\n",
    "                .float() \\\n",
    "                .rename('buildings')\n",
    "\n",
    "            building_percentage = buildings \\\n",
    "                .reproject(crs=crs, scale=1) \\\n",
    "                .reduceResolution(reducer=ee.Reducer.mean(), maxPixels=1000) \\\n",
    "                .reproject(crs=crs, scale=10) \\\n",
    "                .rename('buildingPercentage')\n",
    "\n",
    "            bf_dl_task = ee.batch.Export.image.toDrive(\n",
    "                image=building_percentage.float(),\n",
    "                region=extent.geometry().getInfo()['coordinates'],\n",
    "                description=f'{aoi_id}_{year}_{month:02d}',\n",
    "                folder=f'multimodal_cd_dataset_{dataset}',\n",
    "                fileNamePrefix=f'buildings_{aoi_id}_{year}_{month:02d}',\n",
    "                scale=10,\n",
    "                crs=crs,\n",
    "                maxPixels=250_000,\n",
    "                fileFormat='GeoTIFF'\n",
    "            )\n",
    "            bf_dl_task.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'train'\n",
    "\n",
    "metadata_file = dataset_path / f'metadata_{dataset}.json'\n",
    "metadata = load_json(metadata_file)\n",
    "aoi_ids = metadata.keys()\n",
    "files_path =  Path('C:/Users/shafner/datasets/multimodal_cd_dataset')\n",
    "\n",
    "for i, aoi_id in enumerate(aoi_ids):\n",
    "    timeseries = metadata[aoi_id]\n",
    "    for timestamp in timeseries:\n",
    "        year = timestamp['year']\n",
    "        month = timestamp['month']\n",
    "        features = ['s1', 's2', 'buildings']\n",
    "        for feature in features:\n",
    "            file = files_path / aoi_id / f'{feature}_{aoi_id}_{year}_{month:02d}.tif'\n",
    "            if file.exists():\n",
    "                folder = files_path / aoi_id / feature\n",
    "                folder.mkdir(exist_ok=True)\n",
    "                shutil.copy(file, folder)\n",
    "                file.unlink()\n",
    "            else:\n",
    "                print(file.stem)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct dataset from downloaded data\n",
    "Make sure to place the downloaded datasets (multimodal_cd_dataset_train and multimodal_cd_dataset_test) in your new_dataset_path folder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'train'\n",
    "\n",
    "metadata_file = sn7_path / f'metadata_{dataset}.json'\n",
    "metadata = load_json(metadata_file)\n",
    "aoi_ids = metadata.keys()\n",
    "files_path =  new_dataset_folder / f'multimodal_cd_dataset_{dataset}'\n",
    "\n",
    "for i, aoi_id in enumerate(aoi_ids):\n",
    "    timeseries = metadata[aoi_id]\n",
    "    for timestamp in timeseries:\n",
    "        year = timestamp['year']\n",
    "        month = timestamp['month']\n",
    "        features = ['s1', 's2', 'buildings']\n",
    "        for feature in features:\n",
    "            file = files_path / f'{feature}_{aoi_id}_{year}_{month:02d}.tif'\n",
    "            if file.exists():\n",
    "                folder_aoi = new_dataset_folder / aoi_id\n",
    "                folder_aoi.mkdir(exist_ok=True)\n",
    "                folder = folder_aoi / feature\n",
    "                folder.mkdir(exist_ok=True)\n",
    "                shutil.copy(file, folder)\n",
    "                file.unlink()\n",
    "            else:\n",
    "                print(file.stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create metadata file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_data = load_json(new_dataset_folder / 'bad_data.json')\n",
    "new_metadata = {}\n",
    "for dataset in ['train', 'test']:\n",
    "    metadata_file = sn7_path / f'metadata_{dataset}.json'\n",
    "    metadata = load_json(metadata_file)\n",
    "    aoi_ids = metadata.keys()\n",
    "    for i, aoi_id in enumerate(aoi_ids):\n",
    "        timeseries = metadata[aoi_id]\n",
    "        new_timeseries = []\n",
    "        for j, timestamp in enumerate(timeseries):\n",
    "            year = timestamp['year']\n",
    "            month = timestamp['month']\n",
    "\n",
    "            s1 = new_dataset_folder / aoi_id / 's1' / f's1_{aoi_id}_{year}_{month:02d}.tif'\n",
    "            s2 = new_dataset_folder / aoi_id / 's2' / f's2_{aoi_id}_{year}_{month:02d}.tif'\n",
    "            buildings = new_dataset_folder / aoi_id / 'buildings' / f'buildings_{aoi_id}_{year}_{month:02d}.tif'\n",
    "            mask = sn7_path / dataset / aoi_id / 'UDM_masks' / f'global_monthly_{year}_{month:02d}_mosaic_{aoi_id}_UDM.tif'\n",
    "            new_timestamp = {\n",
    "                'aoi_id': aoi_id, 'year': year, 'month': month,\n",
    "                's1': True if s1.exists() and not j in bad_data[aoi_id]['S1'] else False,\n",
    "                's2': True if s2.exists() and not j in bad_data[aoi_id]['S2'] else False,\n",
    "                'buildings': buildings.exists(),\n",
    "                'masked': mask.exists(),\n",
    "            }\n",
    "            new_timeseries.append(new_timestamp)\n",
    "        new_metadata[aoi_id] = new_timeseries\n",
    "\n",
    "new_metadata_file = new_dataset_folder / 'metadata.json'\n",
    "write_json(new_metadata_file, new_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = Path('C:/Users/shafner/datasets/multimodal_cd_dataset')\n",
    "metadata = load_json(dataset_folder / 'metadata.json')\n",
    "data = {}\n",
    "print(len(metadata.keys()))\n",
    "for aoi_id in sorted(metadata.keys()):\n",
    "    data[aoi_id] = {'S1': [], 'S2': []}\n",
    "\n",
    "out_file = dataset_folder / 'bad_data.json'\n",
    "write_json(out_file, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
